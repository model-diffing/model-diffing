data:
  sequence_iterator:
    classname: CommonCorpusTokenSequenceIterator
    kwargs:
      sequence_length: 256
  sequence_shuffle_buffer_size: 16_384
  activations_harvester:
    llms:
      models:
        - name: EleutherAI/pythia-160M
          revision: step142000
        - name: EleutherAI/pythia-160M
          revision: step143000 # last checkpoint
    layer_indices_to_harvest:
      - 8
    harvest_batch_size: 4
  activations_shuffle_buffer_size: 8_192
  cc_training_batch_size: 512
wandb:
  name: topk_crosscoder_pythia_160M
crosscoder:
  hidden_dim: 6144
  k: 20
train:
  optimizer:
    initial_learning_rate: 5e-5
    last_pct_of_steps: 0.2
  num_steps: 100_000
  save_dir: './.checkpoints/topk_crosscoder_pythia_160M'
  save_every_n_steps: 1000
  log_every_n_steps: 50
