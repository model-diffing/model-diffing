data:
  token_sequence_loader:
    type: HuggingfaceTextDatasetTokenSequenceLoader
    hf_dataset_name: "monology/pile-uncopyrighted"
    sequence_length: 2048
  activations_harvester:
    llms:
      - name: EleutherAI/pythia-160M
        revision: step143000 # last checkpoint
    harvesting_batch_size: 32
crosscoder:
  n_latents: 4096
  jumprelu:
    backprop_through_jumprelu_input: true
  initial_approx_firing_pct: 0.5 # WARNING(oli): very uncertain this is a good default!
train:
  num_steps: 60_000
  batch_size: 65_536
  gradient_accumulation_steps_per_batch: 16  # for a minibatch size of 4096
  save_every_n_steps: 2500
  log_every_n_steps: 20
  c: 4.0
  final_lambda_s: 12.0
  lambda_p: 0.000003 # 3e-6
experiment_name: oli_jumprelu_sliding_window_v2
hookpoints:
  - "blocks.0.hook_resid_post"
  - "blocks.1.hook_resid_post"
  - "blocks.2.hook_resid_post"