data:
  token_sequence_loader:
    type: MathDatasetTokenSequenceLoader
    max_sequence_length: 1024
    include_base_answers: true
    include_reasoning_answers: true
  activations_harvester:
    llms:
      - base_archicteture_name: Qwen/Qwen2.5-1.5B
        hf_model_name: Qwen/Qwen2.5-Math-1.5B
      - base_archicteture_name: Qwen/Qwen2.5-1.5B
        hf_model_name: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B
    harvesting_batch_size: 32
    # inference_dtype: bfloat16
  n_tokens_for_norm_estimate: 40_000
crosscoder:
  hidden_dim: 65_536
  n_shared_latents: 4096 # 1/16 explicitly shared latents
  jumprelu:
    backprop_through_jumprelu_input: true
  initial_approx_firing_pct: 0.2
  n_tokens_for_threshold_setting: 40_000
train:
  num_steps: 10_000
  batch_size: 32_768
  gradient_accumulation_steps_per_batch: 8 # for a minibatch size of 4096
  save_every_n_steps: 1000
  log_every_n_steps: 4
  c: 4.0
  final_lambda_s: 0.5
  final_lambda_f: 5.0
  lambda_p: 0.000003
experiment_name: base
hookpoint: "blocks.20.hook_resid_post"
wandb:
  project: memory_leak_debug