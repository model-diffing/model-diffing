data:
  token_sequence_loader:
    type: HuggingfaceTextDatasetTokenSequenceLoader
    hf_dataset_name: monology/pile-uncopyrighted
    sequence_length: 512
    shuffle_buffer_size: 30_000
  activations_harvester:
    llms:
      - name: EleutherAI/pythia-160M
        revision: step143000
    inference_dtype: bfloat16
    harvesting_batch_size: 64
  n_tokens_for_norm_estimate: 100_000
crosscoder:
  hidden_dim: 65_536
  jumprelu:
    backprop_through_jumprelu_input: true
    log_threshold_init: -10
  initial_approx_firing_pct: 0.3 # WARNING(oli): very uncertain this is a good default!
  n_tokens_for_threshold_setting: 100_000
train:
  optimizer:
    type: adam
    learning_rate: 0.0004
    betas: [0.0, 0.999]
  num_steps: 10000
  batch_size: 256
  gradient_accumulation_steps_per_batch: 1
  save_every_n_steps: 1000
  upload_saves_to_wandb: false
  log_every_n_steps: 1
  c: 4.0
  final_lambda_s: 3.0
  lambda_p: 0.000003
experiment_name: basic_nobias
hookpoints: ["blocks.8.hook_resid_post"]
wandb:
  project: no_bias_jr
bias: false