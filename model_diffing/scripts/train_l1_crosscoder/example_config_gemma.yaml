data:
  activations_iterator:
    layer_indices_to_harvest:
      - 8
    harvest_batch_size: 4
    sequence_tokens_iterator:
      name: connors_gemma
  shuffle_buffer_size: 32_768
  batch_size: 4096
llms:
  models:
    - name: google/gemma-2b
    - name: google/gemma-2b-it
wandb:
  name: l1_crosscoder_gemma_2b
crosscoder:
  hidden_dim: 16_384
train:
  learning_rate:
    initial_learning_rate: 5e-5
    last_pct_of_steps: 0.2
  lambda_max: 5
  lambda_n_steps: 1000
  num_steps: 100_000
  save_dir: './checkpoints/l1_crosscoder_gemma_2b'
  save_every_n_steps: 1000
  log_every_n_steps: 50
