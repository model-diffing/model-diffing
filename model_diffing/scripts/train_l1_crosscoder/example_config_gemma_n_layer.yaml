data:
  sequence_iterator:
    classname: ConnorGemma2TokenSequenceLoader
  sequence_shuffle_buffer_size: 16_384
  activations_harvester:
    layer_indices_to_harvest: [12, 15, 17, 18]
    harvest_batch_size: 1
  activations_shuffle_buffer_size: 4_096
  cc_training_batch_size: 1024
llms:
  models:
    - name: google/gemma-2-2b
    - name: google/gemma-2-2b-it
wandb:
  name: l1_gemma_base_it_4_layers
crosscoder:
  hidden_dim: 16_384
train:
  optimizer:
    initial_learning_rate: 5e-5
    last_pct_of_steps: 0.2
  l1_coef_max: 5
  l1_coef_n_steps: 1000
  num_steps: 100_000
  save_dir: './.checkpoints/l1_gemma_base_it_4_layers'
  save_every_n_steps: 1000
  log_every_n_steps: 100
