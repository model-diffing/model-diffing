seed: 42
dtype: float32
llms:
  - name: google/gemma-2-2b
    revision: null
    d_model : 2304
  - name: google/gemma-2-2b-it
    revision: null
    d_model : 2304
layer_indices_to_harvest:
  - 14
dataset:
  hf_dataset: PleIAs/common_corpus
  cache_dir: "/h/319/owen/.cache/huggingface/transformers" #".cache"
  sequence_length: 1024 #256
  harvest_batch_size: 2 #32
  shuffle_buffer_size: 16384 #32_768
  n_batches_for_norm_estimate: 10
crosscoder:
  hidden_dim: 16384 #which is wht is used wi connors 6144
train:
  learning_rate:
    initial_learning_rate: 5e-5
    last_pct_of_steps: 0.2
  lambda_max: 7.5
  lambda_n_steps: 1000
  batch_size: 4096
  num_steps: 100_000
  save_dir: './checkpoints/l1_crosscoder'
  save_every_n_steps: 250
  log_every_n_steps: 50
wandb:
  project: model-diffing
  entity: mars-model-diffing
