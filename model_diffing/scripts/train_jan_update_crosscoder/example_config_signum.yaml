data:
  token_sequence_loader:
    type: HuggingfaceTextDatasetTokenSequenceLoader
    hf_dataset_name: "monology/pile-uncopyrighted"
    sequence_length: 2048
    shuffle_buffer_size: 4096
  activations_harvester:
    llms:
      - name: EleutherAI/pythia-160M
        revision: step143000
    harvesting_batch_size: 4
    cache_mode: "cache"
  n_tokens_for_norm_estimate: 10_000
crosscoder:
  hidden_dim: 6144
  jumprelu:
    backprop_through_jumprelu_input: true
  initial_approx_firing_pct: 0.3 # WARNING(oli): very uncertain this is a good default!
train:
  optimizer:
    type: schedule_free_signum
    learning_rate: 0.0006
    momentum: 0.95
  num_steps: 3_000
  batch_size: 512
  save_every_n_steps: 9999999999
  log_every_n_steps: 10
  c: 4.0
  final_lambda_s: 1.5
  lambda_p: 0.000003
experiment_name: pythia-160M-10k-signum
hookpoints: ["blocks.8.hook_resid_post"]